name: KV Store & Analytics Sync

on:
  push:
    branches: [main]
    paths:
      - 'context/**'
      - '.ctxrc.yaml'
      - 'performance.yaml'
  
  pull_request:
    paths:
      - 'context/**'
      - 'context_kv.py'
      - 'context_analytics.py'
  
  schedule:
    # Run daily at 2 AM UTC
    - cron: '0 2 * * *'
  
  workflow_dispatch:
    inputs:
      generate_reports:
        description: 'Generate analytics reports'
        required: false
        default: 'true'
      export_data:
        description: 'Export analytics data'
        required: false
        default: 'false'

permissions:
  contents: read
  
jobs:
  kv-analytics:
    name: KV Store & Analytics Sync
    runs-on: ubuntu-latest
    
    services:
      redis:
        image: redis:7-alpine
        ports:
          - 6379:6379
        options: >-
          --health-cmd "redis-cli ping"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
      
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'
      
      - name: Cache pip dependencies
        uses: actions/cache@v3
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-${{ hashFiles('requirements.txt') }}
          restore-keys: |
            ${{ runner.os }}-pip-
      
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
      
      - name: Create DuckDB directory
        run: |
          mkdir -p context/.duckdb
      
      - name: Test KV Store connections
        run: |
          python context_kv.py test-connection --verbose
      
      - name: Process context metrics
        id: process-metrics
        run: |
          echo "Processing context changes..."
          
          # Record deployment event
          python scripts/record_deployment_event.py
          
          # Process document metrics
          yaml_files=$(find context -name "*.yaml" -type f)
          if [ -n "$yaml_files" ]; then
            python scripts/process_document_metrics.py $yaml_files
          else
            echo "No YAML files found to process"
          fi
      
      - name: Generate analytics reports
        if: github.event.inputs.generate_reports != 'false'
        run: |
          echo "## Analytics Reports" > analytics_report.md
          echo "" >> analytics_report.md
          echo "**Generated:** $(date -u '+%Y-%m-%d %H:%M:%S UTC')" >> analytics_report.md
          echo "" >> analytics_report.md
          
          # Generate all reports with proper error handling
          if python context_analytics.py analyze --days 30 --report-type all >> analytics_report.md 2>&1; then
            echo "✓ Analytics reports generated successfully" >> analytics_report.md
          else
            echo "⚠️ Warning: Some analytics reports failed to generate" >> analytics_report.md
            echo "Check the logs for details" >> analytics_report.md
          fi
          
          # Show recent activity
          echo "" >> analytics_report.md
          echo "### Recent Activity (24h)" >> analytics_report.md
          python context_kv.py activity-summary --hours 24 >> analytics_report.md 2>&1 || true
      
      - name: Export analytics data
        if: github.event.inputs.export_data == 'true'
        run: |
          mkdir -p analytics_export
          python context_analytics.py export --output-dir analytics_export --format parquet || \
          python context_analytics.py export --output-dir analytics_export --format csv
      
      - name: Upload analytics report
        if: always() && github.event.inputs.generate_reports != 'false'
        uses: actions/upload-artifact@v4
        with:
          name: analytics-report
          path: analytics_report.md
          retention-days: 30
      
      - name: Upload exported data
        if: github.event.inputs.export_data == 'true'
        uses: actions/upload-artifact@v4
        with:
          name: analytics-export
          path: analytics_export/
          retention-days: 7
      
      - name: Cache metrics summary
        if: success()
        run: |
          # Generate metrics summary for caching
          python -c "
          from context_kv import ContextKV
          import json
          
          kv = ContextKV()
          if kv.connect():
              summary = kv.get_recent_activity(hours=24)
              with open('metrics_summary.json', 'w') as f:
                  json.dump(summary, f, indent=2)
              kv.close()
          "
          
          # Display summary
          if [ -f metrics_summary.json ]; then
            echo "### Metrics Summary"
            cat metrics_summary.json
          fi
      
      - name: Performance monitoring
        if: always()
        run: |
          # Record workflow performance metrics
          python -c "
          from context_kv import ContextKV, MetricEvent
          from datetime import datetime
          
          kv = ContextKV()
          if kv.connect():
              # Record workflow duration
              metric = MetricEvent(
                  timestamp=datetime.utcnow(),
                  metric_name='workflow.duration',
                  value=${{ job.duration || 0 }},
                  tags={
                      'workflow': 'kv-analytics-sync',
                      'status': '${{ job.status }}'
                  }
              )
              kv.redis.record_metric(metric)
              kv.duckdb.insert_metrics([metric])
              kv.close()
          "