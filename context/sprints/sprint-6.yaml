schema_version: 1.0.0
document_type: sprint
id: "sprint-6-autonomous-pipeline"
title: "Sprint 6 - Build Autonomous Software Development Pipeline with Modal OAuth Architecture"
status: planning
created_date: "2025-01-31"
sprint_number: 6
start_date: "2025-02-01"
end_date: "2025-03-21"
last_modified: "2025-02-01"
last_referenced: "2025-02-01"

goals:
  - "Build autonomous software development pipeline from GitHub issue to PR with 80%+ automation rate"
  - "Implement confidence-based decision making with self-healing capabilities"
  - "Create continuous learning system that improves with every issue resolved"
  - "Transform workflow-issue system into serverless Modal architecture with OAuth"
  - "Enable predictive validation and parallel solution exploration"
  - "Achieve 50% reduction in time-to-resolution through intelligent automation"
  - "Implement zero-downtime migration with comprehensive rollback capabilities"

phases:
  - phase: 0
    name: "Proof of Concept & Security Foundation"
    status: pending
    priority: critical
    component: poc-security
    description: |
      Build minimal OAuth + Modal prototype to validate architecture
      and establish secure credential management foundation
    duration_days: 5
    tasks:
      - title: "Build OAuth + Modal Proof of Concept"
        description: |
          Create minimal working prototype to validate OAuth flow with Modal
          before committing to full migration.

          ## Acceptance Criteria
          - [ ] Single Modal function with OAuth authentication working
          - [ ] End-to-end OAuth flow validated (login, refresh, API calls)
          - [ ] Performance benchmarks established (cold start, latency)
          - [ ] Cost analysis completed with projected usage
          - [ ] Security audit of OAuth implementation
          - [ ] Decision checkpoint: proceed or pivot

          ## Implementation Notes
            ```python
            # Minimal PoC structure
            @modal.function(
                secrets=[modal.Secret.from_name("claude-oauth-poc")],
                image=modal.Image.debian_slim().pip_install("anthropic", "authlib")
            )
            def poc_oauth_function(prompt: str) -> dict:
                # Test OAuth flow
                oauth_client = ClaudeOAuthClient(get_secure_credentials())
                response = oauth_client.chat([{"role": "user", "content": prompt}])
                return {"success": True, "response": response.content}
            ```
        labels:
          - "sprint-6"
          - "phase-0"
          - "poc"
          - "critical-path"
        dependencies: []
        estimate: "16 hours"
        assignee: "architecture-lead"

      - title: "Implement Secure Credential Management"
        description: |
          Replace file-based credential storage with OS-native secure storage
          using keyring/keychain integration.

          ## Acceptance Criteria
          - [ ] OS keychain integration implemented (macOS, Linux, Windows)
          - [ ] Encrypted credential storage with automatic migration
          - [ ] Credential rotation mechanism implemented
          - [ ] Audit logging for all credential access
          - [ ] Documentation for credential setup process

          ## Implementation Notes
            ```python
            import keyring
            from cryptography.fernet import Fernet
            import platform
            
            class SecureCredentialManager:
                def __init__(self):
                    self.service_name = "claude-modal-oauth"
                    self._ensure_encryption_key()
                
                def get_credentials(self):
                    # Use OS-specific secure storage
                    access_token = keyring.get_password(self.service_name, "access_token")
                    refresh_token = keyring.get_password(self.service_name, "refresh_token")
                    
                    if not access_token:
                        # Migrate from old file-based storage if exists
                        self._migrate_legacy_credentials()
                    
                    return {
                        "access_token": self._decrypt(access_token),
                        "refresh_token": self._decrypt(refresh_token)
                    }
                
                def _ensure_encryption_key(self):
                    # Store encryption key in OS keychain
                    key = keyring.get_password(self.service_name, "encryption_key")
                    if not key:
                        key = Fernet.generate_key().decode()
                        keyring.set_password(self.service_name, "encryption_key", key)
                    self.cipher = Fernet(key.encode())
            ```
        labels:
          - "sprint-6"
          - "phase-0"
          - "security"
          - "critical-path"
        dependencies: []
        estimate: "12 hours"
        assignee: "security-engineer"

      - title: "Design State Management Architecture"
        description: |
          Design and implement state management solution for stateless
          Modal functions using Redis or external storage.

          ## Acceptance Criteria
          - [ ] State management design documented
          - [ ] Modal Redis integration tested
          - [ ] State persistence across function invocations
          - [ ] Concurrent state access handling
          - [ ] State recovery mechanisms

          ## Implementation Notes
            ```python
            from modal import Image, Secret, Redis
            import json
            from typing import Optional
            
            # Modal app with Redis
            app = modal.App("workflow-oauth")
            redis_image = Image.debian_slim().pip_install("redis", "hiredis")
            
            @app.function(
                image=redis_image,
                secrets=[modal.Secret.from_name("redis-config")]
            )
            async def manage_workflow_state(
                issue_number: int, 
                operation: str, 
                data: Optional[dict] = None
            ) -> dict:
                redis_client = Redis()
                key = f"workflow:state:{issue_number}"
                
                if operation == "get":
                    state = await redis_client.get(key)
                    return json.loads(state) if state else {}
                elif operation == "set":
                    await redis_client.set(key, json.dumps(data), ex=86400)  # 24h TTL
                    return {"success": True}
                elif operation == "update":
                    # Atomic update with optimistic locking
                    # Implementation details...
            ```
        labels:
          - "sprint-6"
          - "phase-0"
          - "architecture"
          - "state-management"
        dependencies: []
        estimate: "10 hours"
        assignee: "backend-engineer"

      - title: "Build AI-Powered Issue Classification System"
        description: |
          Create intelligent issue classification that determines autonomy
          eligibility and complexity scoring.

          ## Acceptance Criteria
          - [ ] Issue classification model with 95%+ accuracy
          - [ ] Complexity scoring algorithm implemented
          - [ ] Autonomy eligibility decision tree
          - [ ] Historical issue analysis for pattern recognition
          - [ ] Real-time classification API endpoint
          - [ ] Human escalation triggers defined

          ## Implementation Notes
            ```python
            from sklearn.ensemble import RandomForestClassifier
            import numpy as np
            from typing import Dict, Tuple
            
            class IssueClassifier:
                def __init__(self):
                    self.complexity_model = self._load_complexity_model()
                    self.autonomy_threshold = 0.85
                    self.vector_db = ChromaDB()
                    
                async def classify_issue(self, issue: Dict) -> IssueClassification:
                    # 1. Extract features
                    features = await self._extract_features(issue)
                    
                    # 2. Get embeddings for semantic search
                    embedding = await self._get_issue_embedding(issue)
                    
                    # 3. Find similar resolved issues
                    similar_issues = await self.vector_db.similarity_search(
                        embedding, 
                        k=10,
                        filter={"status": "resolved"}
                    )
                    
                    # 4. Calculate complexity score
                    complexity_score = self.complexity_model.predict_proba(features)[0][1]
                    
                    # 5. Determine autonomy eligibility
                    autonomy_score = await self._calculate_autonomy_score(
                        issue, 
                        similar_issues,
                        complexity_score
                    )
                    
                    # 6. Classification decision
                    return IssueClassification(
                        type=self._determine_issue_type(features),
                        complexity=complexity_score,
                        autonomy_eligible=autonomy_score > self.autonomy_threshold,
                        autonomy_score=autonomy_score,
                        suggested_approach=self._suggest_approach(similar_issues),
                        escalation_reasons=self._get_escalation_reasons(autonomy_score)
                    )
                
                async def _calculate_autonomy_score(self, issue, similar_issues, complexity):
                    # Factors for autonomy decision
                    factors = {
                        "complexity": 1 - complexity,  # Lower complexity = higher autonomy
                        "similarity_to_resolved": len(similar_issues) / 10,
                        "test_coverage_available": await self._check_test_coverage(issue),
                        "breaking_change_risk": 1 - await self._assess_breaking_change_risk(issue),
                        "security_sensitive": 1 - await self._is_security_sensitive(issue)
                    }
                    
                    # Weighted average
                    weights = {
                        "complexity": 0.3,
                        "similarity_to_resolved": 0.25,
                        "test_coverage_available": 0.2,
                        "breaking_change_risk": 0.15,
                        "security_sensitive": 0.1
                    }
                    
                    return sum(factors[k] * weights[k] for k in factors)
            
            @modal.function(
                schedule=modal.Period(minutes=5),
                image=modal.Image.debian_slim().pip_install(
                    "scikit-learn", "numpy", "chromadb", "anthropic"
                )
            )
            async def autonomous_issue_monitor():
                classifier = IssueClassifier()
                github = GitHubAppClient()
                
                # Get new issues
                new_issues = await github.get_unprocessed_issues()
                
                for issue in new_issues:
                    classification = await classifier.classify_issue(issue)
                    
                    if classification.autonomy_eligible:
                        # Trigger autonomous workflow
                        await trigger_autonomous_workflow.spawn(
                            issue_number=issue["number"],
                            classification=classification
                        )
                    else:
                        # Notify human with reasons
                        await github.add_labels(
                            issue["number"], 
                            ["needs-human-review"] + classification.escalation_reasons
                        )
                        await github.comment(
                            issue["number"],
                            f"This issue requires human review. Reasons: {classification.escalation_reasons}"
                        )
            ```
        labels:
          - "sprint-6"
          - "phase-0"
          - "ai-classification"
          - "autonomous-pipeline"
        dependencies:
          - "Build OAuth + Modal Proof of Concept"
        estimate: "16 hours"
        assignee: "ai-engineer"

      - title: "Implement Learning Database and Vector Store"
        description: |
          Set up vector database for semantic search and learning from
          past issue resolutions.

          ## Acceptance Criteria
          - [ ] Vector database deployed (Pinecone/Chroma/Weaviate)
          - [ ] Issue embedding generation pipeline
          - [ ] Solution pattern storage schema
          - [ ] Outcome tracking database
          - [ ] Learning feedback loop infrastructure
          - [ ] Performance benchmarks established

          ## Implementation Notes
            ```python
            from chromadb import Client
            from sentence_transformers import SentenceTransformer
            import modal
            
            class LearningDatabase:
                def __init__(self):
                    self.vector_client = Client()
                    self.outcome_db = PostgresClient()
                    self.embedder = SentenceTransformer('all-MiniLM-L6-v2')
                    
                    # Create collections
                    self.issues_collection = self.vector_client.create_collection(
                        "issues",
                        metadata={"hnsw:space": "cosine"}
                    )
                    
                    self.solutions_collection = self.vector_client.create_collection(
                        "solutions",
                        metadata={"hnsw:space": "cosine"}
                    )
                
                async def store_issue_resolution(self, issue_id: str, resolution: Dict):
                    # 1. Generate embeddings
                    issue_embedding = self.embedder.encode(resolution["issue_text"])
                    solution_embedding = self.embedder.encode(resolution["solution_text"])
                    
                    # 2. Store in vector DB
                    self.issues_collection.add(
                        embeddings=[issue_embedding],
                        documents=[resolution["issue_text"]],
                        metadatas=[{
                            "issue_id": issue_id,
                            "complexity": resolution["complexity"],
                            "resolution_time": resolution["time_taken"],
                            "success": resolution["success"]
                        }],
                        ids=[issue_id]
                    )
                    
                    self.solutions_collection.add(
                        embeddings=[solution_embedding],
                        documents=[resolution["solution_text"]],
                        metadatas=[{
                            "issue_id": issue_id,
                            "approach": resolution["approach"],
                            "files_changed": resolution["files_changed"],
                            "tests_added": resolution["tests_added"]
                        }],
                        ids=[f"solution_{issue_id}"]
                    )
                    
                    # 3. Store outcome for learning
                    await self.outcome_db.insert("outcomes", {
                        "issue_id": issue_id,
                        "timestamp": datetime.utcnow(),
                        "metrics": resolution["metrics"],
                        "human_feedback": resolution.get("human_feedback"),
                        "auto_merged": resolution.get("auto_merged", False)
                    })
                
                async def find_similar_solutions(self, issue_text: str, k: int = 5):
                    embedding = self.embedder.encode(issue_text)
                    
                    results = self.issues_collection.query(
                        query_embeddings=[embedding],
                        n_results=k,
                        where={"success": True}  # Only successful resolutions
                    )
                    
                    # Enrich with solution details
                    enriched_results = []
                    for i, issue_id in enumerate(results["ids"][0]):
                        solution = self.solutions_collection.get(
                            ids=[f"solution_{issue_id}"]
                        )
                        
                        enriched_results.append({
                            "issue": results["documents"][0][i],
                            "solution": solution["documents"][0],
                            "metadata": {
                                **results["metadatas"][0][i],
                                **solution["metadatas"][0]
                            },
                            "similarity": 1 - results["distances"][0][i]
                        })
                    
                    return enriched_results
            
            @modal.function(
                image=modal.Image.debian_slim().pip_install(
                    "chromadb", "sentence-transformers", "psycopg2-binary"
                ),
                gpu="T4"  # For embedding generation
            )
            async def index_historical_issues():
                """One-time indexing of historical issues for learning"""
                db = LearningDatabase()
                github = GitHubAppClient()
                
                # Get all resolved issues from past 6 months
                historical_issues = await github.get_resolved_issues(
                    since=datetime.now() - timedelta(days=180)
                )
                
                for issue in historical_issues:
                    # Extract resolution details from PR
                    resolution = await extract_resolution_details(issue)
                    await db.store_issue_resolution(issue["number"], resolution)
                
                print(f"Indexed {len(historical_issues)} historical issues")
            ```
        labels:
          - "sprint-6"
          - "phase-0"
          - "learning-system"
          - "vector-database"
        dependencies:
          - "Design State Management Architecture"
        estimate: "14 hours"
        assignee: "data-engineer"

phases:
  - phase: 1
    name: "Infrastructure Setup with OAuth"
    status: pending
    priority: blocking
    component: modal-infrastructure
    description: |
      Set up Modal.com infrastructure with Claude OAuth authentication
      instead of API keys, mirroring GitHub Actions @claude pattern
    duration_days: 7
    tasks:
      - title: "Set up Modal Application with Secure OAuth Infrastructure"
        description: |
          Create the foundational Modal application with secure Claude OAuth
          authentication, automated token rotation, and monitoring.

          ## Acceptance Criteria
          - [ ] Modal application created with secure OAuth structure
          - [ ] Secure credential retrieval using OS keychain (not file-based)
          - [ ] Automated OAuth token refresh with concurrent request handling
          - [ ] Token rotation scheduled function implemented
          - [ ] Monitoring and alerting for OAuth failures
          - [ ] Rate limiting implementation for API calls
          - [ ] Development and production environment separation

          ## Implementation Notes
            ```python
            import modal
            from datetime import datetime, timedelta
            import asyncio
            from authlib.integrations.httpx_client import AsyncOAuth2Client
            
            app = modal.App("workflow-oauth")
            
            # Secure credential manager (from Phase 0)
            credential_manager = SecureCredentialManager()
            
            class OAuthTokenManager:
                def __init__(self):
                    self._lock = asyncio.Lock()
                    self._token_cache = {}
                    
                async def get_valid_token(self):
                    async with self._lock:
                        # Check if token is still valid
                        if self._is_token_valid():
                            return self._token_cache["access_token"]
                        
                        # Refresh token
                        return await self._refresh_token()
                
                async def _refresh_token(self):
                    creds = credential_manager.get_credentials()
                    client = AsyncOAuth2Client(
                        client_id=oauth_config["client_id"],
                        token=creds
                    )
                    
                    new_token = await client.refresh_token(
                        "https://api.anthropic.com/oauth/token",
                        refresh_token=creds["refresh_token"]
                    )
                    
                    # Update secure storage
                    credential_manager.store_credentials(new_token)
                    self._token_cache = new_token
                    
                    return new_token["access_token"]
            
            # Scheduled token rotation
            @app.function(schedule=modal.Period(hours=4))
            def rotate_oauth_tokens():
                manager = OAuthTokenManager()
                asyncio.run(manager._refresh_token())
                print(f"Tokens rotated at {datetime.utcnow()}")
            ```
        labels:
          - "sprint-6"
          - "phase-1"
          - "oauth-infrastructure"
          - "priority-blocking"
        dependencies: []
        estimate: "10 hours"
        assignee: "engineering-lead"

      - title: "Implement Claude SDK OAuth Client with Rate Limiting"
        description: |
          Create a robust wrapper for Claude SDK that uses OAuth tokens with
          automatic refresh, rate limiting, and usage tracking.

          ## Acceptance Criteria
          - [ ] Claude OAuth client wrapper with thread-safe token management
          - [ ] Automatic token refresh with retry logic
          - [ ] Rate limiting to prevent API exhaustion
          - [ ] Circuit breaker for API failures
          - [ ] Usage tracking and cost estimation
          - [ ] Comprehensive error handling and logging
          - [ ] Support for streaming responses

          ## Implementation Notes
            ```python
            from anthropic import Anthropic, APIError
            import httpx
            from tenacity import retry, stop_after_attempt, wait_exponential
            from circuitbreaker import circuit
            import asyncio
            from collections import deque
            from datetime import datetime, timedelta
            
            class ClaudeOAuthClient:
                def __init__(self, token_manager: OAuthTokenManager):
                    self.token_manager = token_manager
                    self.rate_limiter = RateLimiter(
                        max_requests=100,
                        time_window=timedelta(minutes=1)
                    )
                    self.usage_tracker = UsageTracker()
                    
                async def _get_client(self):
                    token = await self.token_manager.get_valid_token()
                    return Anthropic(
                        http_client=httpx.AsyncClient(
                            headers={"Authorization": f"Bearer {token}"},
                            timeout=30.0
                        ),
                        base_url="https://api.anthropic.com/v1"
                    )
                
                @retry(
                    stop=stop_after_attempt(3),
                    wait=wait_exponential(multiplier=1, min=4, max=10)
                )
                @circuit(failure_threshold=5, recovery_timeout=60)
                async def chat(self, messages, model="claude-3-opus-20240229", **kwargs):
                    # Rate limiting check
                    await self.rate_limiter.acquire()
                    
                    try:
                        client = await self._get_client()
                        response = await client.messages.create(
                            model=model,
                            messages=messages,
                            **kwargs
                        )
                        
                        # Track usage
                        self.usage_tracker.record(
                            model=model,
                            input_tokens=response.usage.input_tokens,
                            output_tokens=response.usage.output_tokens
                        )
                        
                        return response
                        
                    except APIError as e:
                        if e.status_code == 401:
                            # Force token refresh
                            await self.token_manager._refresh_token()
                            raise  # Let retry handle it
                        elif e.status_code == 429:
                            # Rate limit hit - back off
                            await asyncio.sleep(60)
                            raise
                        else:
                            raise
                
                async def stream_chat(self, messages, model="claude-3-opus-20240229", **kwargs):
                    # Streaming support with rate limiting
                    await self.rate_limiter.acquire()
                    
                    client = await self._get_client()
                    async with client.messages.stream(
                        model=model,
                        messages=messages,
                        **kwargs
                    ) as stream:
                        async for chunk in stream:
                            yield chunk
            
            class RateLimiter:
                def __init__(self, max_requests, time_window):
                    self.max_requests = max_requests
                    self.time_window = time_window
                    self.requests = deque()
                    self._lock = asyncio.Lock()
                
                async def acquire(self):
                    async with self._lock:
                        now = datetime.utcnow()
                        # Remove old requests outside time window
                        while self.requests and self.requests[0] < now - self.time_window:
                            self.requests.popleft()
                        
                        if len(self.requests) >= self.max_requests:
                            sleep_time = (self.requests[0] + self.time_window - now).total_seconds()
                            await asyncio.sleep(sleep_time)
                            return await self.acquire()
                        
                        self.requests.append(now)
            ```
        labels:
          - "sprint-6"
          - "phase-1"
          - "oauth-client"
          - "priority-high"
        dependencies:
          - "Set up Modal Application with Claude OAuth Infrastructure"
        estimate: "8 hours"
        assignee: "backend-engineer"

      - title: "Implement Secure GitHub Webhook Handler with @claude Mention Support"
        description: |
          Create secure GitHub webhook endpoint with signature validation,
          @claude mention detection, and natural language command parsing.

          ## Acceptance Criteria
          - [ ] GitHub webhook endpoint with cryptographic signature validation
          - [ ] Robust @claude mention detection and command extraction
          - [ ] Natural language command parsing with fallback handling
          - [ ] Idempotency key support to prevent duplicate processing
          - [ ] Webhook event replay protection
          - [ ] Comprehensive audit logging
          - [ ] Error responses that don't leak sensitive information

          ## Implementation Notes
            ```python
            import hmac
            import hashlib
            from fastapi import Request, Response, HTTPException
            import re
            from datetime import datetime, timedelta
            
            @modal.web_endpoint(method="POST")
            async def github_webhook(request: Request):
                # Validate GitHub signature
                signature = request.headers.get("X-Hub-Signature-256")
                if not signature:
                    raise HTTPException(status_code=401, detail="Missing signature")
                
                body = await request.body()
                secret = os.environ["GITHUB_WEBHOOK_SECRET"]
                
                expected_signature = "sha256=" + hmac.new(
                    secret.encode(),
                    body,
                    hashlib.sha256
                ).hexdigest()
                
                if not hmac.compare_digest(signature, expected_signature):
                    # Log security event
                    await log_security_event("Invalid webhook signature")
                    raise HTTPException(status_code=401, detail="Invalid signature")
                
                # Check delivery ID for idempotency
                delivery_id = request.headers.get("X-GitHub-Delivery")
                if await is_duplicate_delivery(delivery_id):
                    return Response(status_code=200, content="Already processed")
                
                payload = await request.json()
                
                # Validate timestamp to prevent replay attacks
                if "timestamp" in payload:
                    event_time = datetime.fromisoformat(payload["timestamp"])
                    if datetime.utcnow() - event_time > timedelta(minutes=5):
                        return Response(status_code=200, content="Event too old")
                
                # Handle different event types
                if payload.get("action") == "created" and "comment" in payload:
                    await handle_comment_created(payload)
                elif payload.get("action") == "opened" and "issue" in payload:
                    await handle_issue_opened(payload)
                
                # Mark delivery as processed
                await mark_delivery_processed(delivery_id)
                
                return Response(status_code=200)
            
            async def handle_comment_created(payload):
                comment_body = payload["comment"]["body"]
                mentions = extract_claude_mentions(comment_body)
                
                for mention in mentions:
                    # Parse command with context awareness
                    command_context = {
                        "issue": payload["issue"],
                        "comment": payload["comment"],
                        "repository": payload["repository"],
                        "sender": payload["sender"]
                    }
                    
                    parsed_command = await parse_claude_command(
                        mention["text"],
                        command_context
                    )
                    
                    # Trigger workflow asynchronously
                    await trigger_workflow.spawn(
                        command=parsed_command,
                        context=command_context,
                        correlation_id=f"{payload['issue']['number']}-{payload['comment']['id']}"
                    )
                    
                    # Post acknowledgment
                    await post_github_comment(
                        repo=payload["repository"]["full_name"],
                        issue_number=payload["issue"]["number"],
                        body=f"👋 @{payload['sender']['login']}, I'm processing your request: `{parsed_command['intent']}`"
                    )
            
            def extract_claude_mentions(text):
                # Handle multiple mentions and quoted text
                pattern = r'@claude\s+([^@]+?)(?=@claude|$)'
                mentions = []
                
                for match in re.finditer(pattern, text, re.IGNORECASE | re.DOTALL):
                    command_text = match.group(1).strip()
                    # Filter out quoted replies
                    if not command_text.startswith('>'):
                        mentions.append({
                            "start": match.start(),
                            "end": match.end(),
                            "text": command_text
                        })
                
                return mentions
            ```
        labels:
          - "sprint-6"
          - "phase-1"
          - "webhook-handler"
          - "claude-mentions"
        dependencies:
          - "Implement Claude SDK OAuth Client with Rate Limiting"
        estimate: "6 hours"
        assignee: "backend-engineer"

      - title: "Migrate from GitHub CLI to GitHub App Authentication"
        description: |
          Replace GitHub CLI (gh) usage with GitHub App installation for
          better authentication in serverless Modal environment.

          ## Acceptance Criteria
          - [ ] GitHub App created with appropriate permissions
          - [ ] Installation token generation implemented
          - [ ] JWT-based authentication for App
          - [ ] All gh CLI commands replaced with API calls
          - [ ] Token caching with automatic refresh
          - [ ] Support for multiple repository installations

          ## Implementation Notes
            ```python
            import jwt
            import time
            from github import Github, GithubIntegration
            from cryptography.hazmat.primitives import serialization
            from cryptography.hazmat.backends import default_backend
            
            class GitHubAppClient:
                def __init__(self, app_id: str, private_key_path: str):
                    self.app_id = app_id
                    self.private_key = self._load_private_key(private_key_path)
                    self._installation_tokens = {}  # Cache by installation_id
                    
                def _load_private_key(self, key_path):
                    with open(key_path, 'rb') as key_file:
                        return serialization.load_pem_private_key(
                            key_file.read(),
                            password=None,
                            backend=default_backend()
                        )
                
                def _generate_jwt(self):
                    # JWT valid for 10 minutes (GitHub max)
                    now = int(time.time())
                    payload = {
                        'iat': now,
                        'exp': now + 600,
                        'iss': self.app_id
                    }
                    
                    return jwt.encode(
                        payload,
                        self.private_key,
                        algorithm='RS256'
                    )
                
                async def get_installation_client(self, repo_full_name: str):
                    # Get installation ID for repository
                    jwt_token = self._generate_jwt()
                    integration = GithubIntegration(
                        integration_id=self.app_id,
                        private_key=self.private_key
                    )
                    
                    # Find installation for this repo
                    installation = integration.get_repo_installation(repo_full_name)
                    
                    # Get or refresh installation token
                    if installation.id not in self._installation_tokens or \
                       self._is_token_expired(self._installation_tokens[installation.id]):
                        token = integration.get_access_token(installation.id)
                        self._installation_tokens[installation.id] = {
                            'token': token.token,
                            'expires_at': token.expires_at
                        }
                    
                    return Github(self._installation_tokens[installation.id]['token'])
                
                # GitHub operations that replace gh CLI
                async def create_pull_request(self, repo_full_name: str, 
                                            title: str, body: str, 
                                            head: str, base: str = "main"):
                    client = await self.get_installation_client(repo_full_name)
                    repo = client.get_repo(repo_full_name)
                    
                    pr = repo.create_pull(
                        title=title,
                        body=body,
                        head=head,
                        base=base
                    )
                    
                    return {
                        "number": pr.number,
                        "url": pr.html_url,
                        "id": pr.id
                    }
                
                async def create_issue_comment(self, repo_full_name: str,
                                             issue_number: int, body: str):
                    client = await self.get_installation_client(repo_full_name)
                    repo = client.get_repo(repo_full_name)
                    issue = repo.get_issue(issue_number)
                    
                    comment = issue.create_comment(body)
                    return comment.id
                
                async def get_issue(self, repo_full_name: str, issue_number: int):
                    client = await self.get_installation_client(repo_full_name)
                    repo = client.get_repo(repo_full_name)
                    issue = repo.get_issue(issue_number)
                    
                    return {
                        "number": issue.number,
                        "title": issue.title,
                        "body": issue.body,
                        "state": issue.state,
                        "labels": [label.name for label in issue.labels]
                    }
            
            # Modal function using GitHub App
            @modal.function(
                secrets=[modal.Secret.from_name("github-app-credentials")],
                image=modal.Image.debian_slim().pip_install("PyGithub", "pyjwt", "cryptography")
            )
            async def github_operation(operation: str, **kwargs):
                app_id = os.environ["GITHUB_APP_ID"]
                private_key = os.environ["GITHUB_APP_PRIVATE_KEY"]
                
                github_client = GitHubAppClient(app_id, private_key)
                
                # Route to appropriate operation
                if operation == "create_pr":
                    return await github_client.create_pull_request(**kwargs)
                elif operation == "comment":
                    return await github_client.create_issue_comment(**kwargs)
                # ... other operations
            ```
        labels:
          - "sprint-6"
          - "phase-1"
          - "github-app"
          - "authentication"
        dependencies:
          - "Implement Secure GitHub Webhook Handler with @claude Mention Support"
        estimate: "12 hours"
        assignee: "devops-engineer"

  - phase: 2
    name: "Autonomous Agent Development"
    status: pending
    priority: high
    component: autonomous-agents
    description: "Build autonomous agents with confidence scoring and self-healing"
    duration_days: 14
    tasks:
      - title: "Build Autonomous Issue Investigator with Confidence Scoring"
        description: |
          Create autonomous issue investigator that uses similar issue
          patterns and provides confidence scores for its findings.

          ## Acceptance Criteria
          - [ ] Autonomous investigation with confidence scoring
          - [ ] Similar issue pattern recognition
          - [ ] Root cause analysis with multiple hypotheses
          - [ ] Automatic scope expansion when needed
          - [ ] Self-validation of investigation results
          - [ ] Escalation triggers for complex issues

          ## Implementation Notes
            ```python
            @modal.function(
                secrets=[modal.Secret.from_name("claude-oauth-tokens")],
                image=modal.Image.debian_slim().pip_install(
                    "anthropic", "httpx", "numpy", "scikit-learn"
                )
            )
            async def autonomous_investigate_issue(
                issue: dict, 
                classification: IssueClassification
            ) -> InvestigationResult:
                # Initialize services
                claude = ClaudeOAuthClient(oauth_token)
                learning_db = LearningDatabase()
                
                # 1. Find similar resolved issues
                similar_issues = await learning_db.find_similar_solutions(
                    issue["title"] + "\n" + issue["body"],
                    k=10
                )
                
                # 2. Generate multiple investigation hypotheses
                hypotheses = []
                
                # Hypothesis from similar issues
                if similar_issues:
                    pattern_hypothesis = await claude.chat([
                        {"role": "system", "content": "You are analyzing patterns in similar issues..."},
                        {"role": "user", "content": f"""
                        Current issue: {issue}
                        Similar resolved issues: {similar_issues}
                        
                        What patterns do you see? What's likely the root cause?
                        """}
                    ])
                    hypotheses.append({
                        "source": "pattern_analysis",
                        "hypothesis": pattern_hypothesis.content,
                        "confidence": calculate_pattern_confidence(similar_issues)
                    })
                
                # Direct investigation hypothesis
                direct_hypothesis = await claude.chat([
                    {"role": "system", "content": "You are an expert issue investigator..."},
                    {"role": "user", "content": f"""
                    Investigate this issue and identify root causes:
                    {issue}
                    
                    Consider:
                    1. Error messages and stack traces
                    2. Recent changes that might be related
                    3. System dependencies
                    4. Environmental factors
                    """}
                ])
                hypotheses.append({
                    "source": "direct_analysis",
                    "hypothesis": direct_hypothesis.content,
                    "confidence": 0.7  # Base confidence
                })
                
                # 3. Validate hypotheses with code analysis
                validated_hypotheses = []
                for hypothesis in hypotheses:
                    validation = await validate_hypothesis(
                        hypothesis,
                        issue,
                        codebase_context
                    )
                    
                    validated_hypotheses.append({
                        **hypothesis,
                        "validation_score": validation.score,
                        "evidence": validation.evidence,
                        "final_confidence": hypothesis["confidence"] * validation.score
                    })
                
                # 4. Select best hypothesis
                best_hypothesis = max(
                    validated_hypotheses, 
                    key=lambda h: h["final_confidence"]
                )
                
                # 5. Determine if we need human escalation
                max_confidence = best_hypothesis["final_confidence"]
                needs_escalation = (
                    max_confidence < 0.7 or
                    classification.complexity > 0.8 or
                    any(trigger in str(issue) for trigger in ESCALATION_TRIGGERS)
                )
                
                # 6. Generate investigation report
                investigation = InvestigationResult(
                    issue_id=issue["number"],
                    root_cause=best_hypothesis["hypothesis"],
                    confidence=max_confidence,
                    evidence=best_hypothesis["evidence"],
                    alternative_hypotheses=[h for h in validated_hypotheses if h != best_hypothesis],
                    recommended_approach=await generate_approach(best_hypothesis),
                    estimated_effort=estimate_effort(best_hypothesis, similar_issues),
                    needs_human_review=needs_escalation,
                    escalation_reasons=get_escalation_reasons(max_confidence, classification)
                )
                
                # 7. Store for learning
                await learning_db.store_investigation(issue["number"], investigation)
                
                return investigation
            
            def calculate_pattern_confidence(similar_issues):
                """Calculate confidence based on similarity and success of past resolutions"""
                if not similar_issues:
                    return 0.0
                
                # Factors: similarity score, number of similar issues, success rate
                avg_similarity = np.mean([issue["similarity"] for issue in similar_issues])
                success_rate = np.mean([issue["metadata"]["success"] for issue in similar_issues])
                volume_factor = min(len(similar_issues) / 5, 1.0)  # Max confidence at 5+ similar
                
                return avg_similarity * success_rate * volume_factor * 0.9  # Max 0.9 from patterns
            ```
        labels:
          - "sprint-6"
          - "phase-2"
          - "oauth-migration"
          - "issue-investigator"
        dependencies:
          - "Implement GitHub Webhook Handler with @claude Mention Support"
        estimate: "10 hours"
        assignee: "ai-engineer"

      - title: "Build Autonomous Implementation Engine with Self-Healing"
        description: |
          Create autonomous implementation engine that generates code,
          validates it, and self-heals when tests fail.

          ## Acceptance Criteria
          - [ ] Autonomous code generation with confidence tracking
          - [ ] Incremental implementation with validation checkpoints
          - [ ] Self-healing when tests fail
          - [ ] Parallel solution exploration
          - [ ] Automatic rollback on confidence drop
          - [ ] Learning from implementation outcomes

          ## Implementation Notes
            ```python
            @modal.function(
                secrets=[modal.Secret.from_name("claude-oauth-tokens")],
                image=modal.Image.debian_slim().pip_install(
                    "anthropic", "pydantic", "gitpython", "pytest", "ast"
                ),
                timeout=1800  # 30 minutes for complex implementations
            )
            async def autonomous_implement(
                investigation: InvestigationResult,
                classification: IssueClassification
            ) -> ImplementationResult:
                claude = ClaudeOAuthClient(oauth_token)
                learning_db = LearningDatabase()
                
                # 1. Generate implementation plan from investigation
                plan = await generate_implementation_plan(
                    investigation,
                    similar_implementations=await learning_db.get_similar_implementations(
                        investigation.issue_id
                    )
                )
                
                # 2. Set up isolated git worktree
                worktree_path = f"/tmp/worktree-{investigation.issue_id}"
                repo = setup_git_worktree(worktree_path, f"fix/{investigation.issue_id}")
                
                # 3. Implement incrementally with validation
                implementation_state = ImplementationState()
                
                for step in plan.steps:
                    # Generate code for this step
                    code_generation = await generate_code_for_step(
                        step,
                        investigation,
                        current_state=implementation_state
                    )
                    
                    # Apply changes
                    files_changed = apply_code_changes(
                        repo,
                        code_generation.changes
                    )
                    
                    # Validate changes
                    validation = await validate_changes(
                        repo,
                        files_changed,
                        step.validation_criteria
                    )
                    
                    if validation.passed:
                        # Update state and continue
                        implementation_state.record_success(step, validation)
                        implementation_state.confidence *= validation.confidence_factor
                    else:
                        # Self-healing: try alternative approach
                        healing_result = await self_heal_implementation(
                            step,
                            validation.errors,
                            implementation_state,
                            max_attempts=3
                        )
                        
                        if healing_result.success:
                            implementation_state.record_healing(step, healing_result)
                        else:
                            # Rollback this step and try different approach
                            repo.index.reset(commit=implementation_state.last_good_commit)
                            
                            # Try alternative implementation strategy
                            alternative = await generate_alternative_approach(
                                step,
                                validation.errors,
                                implementation_state.attempted_approaches
                            )
                            
                            if alternative:
                                plan.replace_step(step, alternative)
                            else:
                                # Escalate to human
                                return ImplementationResult(
                                    success=False,
                                    needs_human_intervention=True,
                                    partial_implementation=implementation_state,
                                    escalation_reason=f"Cannot implement step: {step.description}"
                                )
                    
                    # Check if confidence is still acceptable
                    if implementation_state.confidence < 0.6:
                        return ImplementationResult(
                            success=False,
                            needs_human_intervention=True,
                            partial_implementation=implementation_state,
                            escalation_reason="Confidence dropped below threshold"
                        )
                
                # 4. Final validation
                final_validation = await comprehensive_validation(repo, plan)
                
                # 5. Create commits with meaningful messages
                commit_messages = await generate_commit_messages(
                    implementation_state,
                    investigation
                )
                
                for commit in commit_messages:
                    repo.index.add(commit.files)
                    repo.index.commit(commit.message)
                
                # 6. Push to remote
                origin = repo.remote("origin")
                origin.push(f"fix/{investigation.issue_id}")
                
                # 7. Store outcome for learning
                await learning_db.store_implementation_outcome(
                    investigation.issue_id,
                    implementation_state,
                    final_validation
                )
                
                return ImplementationResult(
                    success=True,
                    branch_name=f"fix/{investigation.issue_id}",
                    files_changed=implementation_state.all_files_changed,
                    tests_added=implementation_state.tests_added,
                    confidence=implementation_state.confidence,
                    implementation_time=implementation_state.elapsed_time,
                    self_healing_count=implementation_state.healing_count
                )
            
            async def self_heal_implementation(step, errors, state, max_attempts):
                """Self-healing mechanism that learns from errors"""
                
                for attempt in range(max_attempts):
                    # Analyze errors with AI
                    error_analysis = await claude.chat([
                        {"role": "system", "content": "You are debugging a failed implementation..."},
                        {"role": "user", "content": f"""
                        Step: {step}
                        Errors: {errors}
                        Current implementation: {state.current_code}
                        
                        What went wrong and how can we fix it?
                        """}
                    ])
                    
                    # Generate fix
                    fix = await generate_fix(error_analysis, step, state)
                    
                    # Apply and test fix
                    validation = await validate_changes(repo, fix.changes, step.validation_criteria)
                    
                    if validation.passed:
                        return HealingResult(
                            success=True,
                            fix_applied=fix,
                            attempts=attempt + 1
                        )
                    
                    # Learn from this failure
                    state.record_failed_approach(fix, validation.errors)
                
                return HealingResult(success=False, attempts=max_attempts)
            ```
        labels:
          - "sprint-6"
          - "phase-2"
          - "oauth-migration"
          - "task-planner"
        dependencies:
          - "Migrate issue-investigator to OAuth-based Modal"
        estimate: "12 hours"
        assignee: "ai-engineer"

      - title: "Build Autonomous Test Validation with Predictive Analysis"
        description: |
          Create autonomous test runner that predicts test failures,
          generates missing tests, and provides confidence scoring.

          ## Acceptance Criteria
          - [ ] Predictive test failure analysis before running tests
          - [ ] Automatic test generation for uncovered code
          - [ ] Intelligent test prioritization
          - [ ] Self-healing test fixes
          - [ ] Production impact prediction
          - [ ] Confidence scoring for test coverage

          ## Implementation Notes
            ```python
            @modal.function(
                secrets=[modal.Secret.from_name("claude-oauth-tokens")],
                image=modal.Image.debian_slim()
                    .pip_install("anthropic", "pytest", "coverage", "ast", "numpy")
                    .run_commands("apt-get update && apt-get install -y docker.io"),
                timeout=1200  # 20 minutes for comprehensive testing
            )
            async def autonomous_test_validation(
                implementation: ImplementationResult,
                investigation: InvestigationResult
            ) -> TestValidationResult:
                claude = ClaudeOAuthClient(oauth_token)
                learning_db = LearningDatabase()
                
                # 1. Predict which tests might fail
                test_predictions = await predict_test_failures(
                    implementation.files_changed,
                    similar_changes=await learning_db.get_similar_test_outcomes(
                        implementation.files_changed
                    )
                )
                
                # 2. Generate missing tests if needed
                coverage_analysis = await analyze_test_coverage(implementation)
                
                if coverage_analysis.uncovered_critical_paths:
                    generated_tests = await generate_missing_tests(
                        coverage_analysis.uncovered_critical_paths,
                        implementation,
                        investigation
                    )
                    
                    # Add generated tests to the repository
                    for test in generated_tests:
                        write_test_file(test.path, test.content)
                        implementation.tests_added.append(test.path)
                
                # 3. Prioritize test execution
                test_priority = await prioritize_tests(
                    all_tests=get_all_tests(),
                    changed_files=implementation.files_changed,
                    predictions=test_predictions
                )
                
                # 4. Run tests in priority order with early stopping
                test_results = TestResults()
                confidence = 1.0
                
                for test_batch in test_priority.batches:
                    batch_results = await run_test_batch(test_batch)
                    test_results.add_batch(batch_results)
                    
                    # Update confidence based on results
                    if batch_results.failures:
                        confidence *= 0.8 ** len(batch_results.failures)
                        
                        # Try to self-heal failing tests
                        for failure in batch_results.failures:
                            healing = await attempt_test_healing(
                                failure,
                                implementation,
                                max_attempts=2
                            )
                            
                            if healing.success:
                                test_results.record_healing(failure, healing)
                                confidence *= 1.1  # Boost confidence for successful healing
                            else:
                                # Analyze if this is a real issue or test problem
                                failure_analysis = await analyze_test_failure(
                                    failure,
                                    implementation,
                                    investigation
                                )
                                
                                if failure_analysis.is_implementation_bug:
                                    # Real bug found
                                    return TestValidationResult(
                                        success=False,
                                        confidence=confidence * 0.5,
                                        test_results=test_results,
                                        bug_found=failure_analysis,
                                        needs_implementation_fix=True
                                    )
                    
                    # Early stopping if confidence too low
                    if confidence < 0.5:
                        break
                
                # 5. Predict production impact
                production_impact = await predict_production_impact(
                    implementation,
                    test_results,
                    historical_incidents=await learning_db.get_production_incidents()
                )
                
                # 6. Generate comprehensive test report
                test_report = await generate_test_report(
                    test_results,
                    coverage_analysis,
                    production_impact,
                    confidence
                )
                
                # 7. Determine if safe to proceed
                safe_to_proceed = (
                    confidence > 0.7 and
                    test_results.pass_rate > 0.95 and
                    production_impact.risk_score < 0.3
                )
                
                # 8. Store outcomes for learning
                await learning_db.store_test_outcome(
                    implementation.issue_id,
                    test_results,
                    production_impact
                )
                
                return TestValidationResult(
                    success=safe_to_proceed,
                    confidence=confidence,
                    test_results=test_results,
                    coverage=coverage_analysis.percentage,
                    production_risk=production_impact.risk_score,
                    test_report=test_report,
                    generated_tests=len(generated_tests) if 'generated_tests' in locals() else 0,
                    healed_tests=len(test_results.healed_tests)
                )
            
            async def attempt_test_healing(failure, implementation, max_attempts):
                """Self-heal failing tests by understanding and fixing the root cause"""
                
                for attempt in range(max_attempts):
                    # Analyze why test is failing
                    failure_analysis = await claude.chat([
                        {"role": "system", "content": "You are analyzing a test failure..."},
                        {"role": "user", "content": f"""
                        Test: {failure.test_name}
                        Error: {failure.error}
                        Stack trace: {failure.stack_trace}
                        
                        Implementation changes: {implementation.summary}
                        
                        Is this a test that needs updating or a real bug?
                        If it's the test, how should we fix it?
                        """}
                    ])
                    
                    if "real bug" in failure_analysis.content.lower():
                        return TestHealingResult(success=False, is_real_bug=True)
                    
                    # Generate test fix
                    test_fix = await generate_test_fix(
                        failure,
                        failure_analysis,
                        implementation
                    )
                    
                    # Apply and verify fix
                    apply_test_fix(test_fix)
                    
                    # Re-run the test
                    rerun_result = await run_single_test(failure.test_name)
                    
                    if rerun_result.passed:
                        return TestHealingResult(
                            success=True,
                            fix_applied=test_fix,
                            attempts=attempt + 1
                        )
                
                return TestHealingResult(success=False, attempts=max_attempts)
            ```
        labels:
          - "sprint-6"
          - "phase-2"
          - "oauth-migration"
          - "test-runner"
        dependencies:
          - "Migrate task-planner to OAuth-based Modal"
        estimate: "14 hours"
        assignee: "qa-engineer"

      - title: "Build Autonomous PR Manager with Auto-Merge Capability"
        description: |
          Create autonomous PR manager that handles creation, review
          responses, and auto-merge decisions with confidence scoring.

          ## Acceptance Criteria
          - [ ] Autonomous PR creation with optimal descriptions
          - [ ] AI-powered code review and response generation
          - [ ] Auto-merge decision making with confidence thresholds
          - [ ] Review comment handling and implementation
          - [ ] Conflict resolution assistance
          - [ ] Post-merge monitoring setup

          ## Implementation Notes
            ```python
            @modal.function(
                secrets=[modal.Secret.from_name("claude-oauth-tokens", "github-app-credentials")],
                image=modal.Image.debian_slim().pip_install(
                    "anthropic", "PyGithub", "unidiff", "gitpython"
                )
            )
            async def autonomous_pr_manager(
                implementation: ImplementationResult,
                test_validation: TestValidationResult,
                investigation: InvestigationResult
            ) -> PRResult:
                claude = ClaudeOAuthClient(oauth_token)
                github = GitHubAppClient()
                learning_db = LearningDatabase()
                
                # 1. Generate optimal PR description
                pr_description = await generate_optimal_pr_description(
                    implementation,
                    test_validation,
                    investigation,
                    similar_prs=await learning_db.get_successful_pr_examples()
                )
                
                # 2. Create PR
                pr = await github.create_pull_request(
                    repo_full_name=get_repo_name(),
                    title=pr_description.title,
                    body=pr_description.body,
                    head=implementation.branch_name,
                    base="main"
                )
                
                # 3. Add metadata labels
                await github.add_labels(pr.number, [
                    f"autonomous-confidence-{int(implementation.confidence * 100)}",
                    f"complexity-{investigation.complexity_category}",
                    "auto-generated"
                ])
                
                # 4. Perform self-review
                self_review = await perform_ai_code_review(
                    pr_diff=await github.get_pr_diff(pr.number),
                    implementation_context=implementation,
                    test_results=test_validation
                )
                
                if self_review.issues_found:
                    # Post review comments
                    for issue in self_review.issues_found:
                        await github.create_review_comment(
                            pr.number,
                            body=issue.description,
                            path=issue.file_path,
                            line=issue.line_number
                        )
                    
                    # Attempt to fix issues autonomously
                    if self_review.auto_fixable:
                        fixes = await generate_review_fixes(
                            self_review.issues_found,
                            implementation
                        )
                        
                        # Apply fixes and push
                        apply_fixes_to_branch(fixes, implementation.branch_name)
                        
                        # Comment about fixes
                        await github.comment(
                            pr.number,
                            f"🤖 I've automatically addressed {len(fixes)} review comments. Please review the updates."
                        )
                
                # 5. Monitor for human reviews
                pr_monitor = PRMonitor(pr.number)
                
                @pr_monitor.on_review_comment
                async def handle_review_comment(comment):
                    # Analyze if we can address the comment
                    analysis = await analyze_review_comment(comment)
                    
                    if analysis.can_address_autonomously:
                        # Generate and apply fix
                        fix = await generate_comment_fix(comment, implementation)
                        apply_fix_to_branch(fix, implementation.branch_name)
                        
                        # Respond to comment
                        await github.reply_to_comment(
                            comment.id,
                            f"✅ I've addressed this in commit {fix.commit_sha}. {fix.explanation}"
                        )
                    else:
                        # Provide helpful context
                        context = await generate_helpful_context(comment, implementation)
                        await github.reply_to_comment(
                            comment.id,
                            f"📝 {context.explanation}\n\n{context.suggestions}"
                        )
                
                # 6. Auto-merge decision
                merge_confidence = await calculate_merge_confidence(
                    implementation,
                    test_validation,
                    self_review,
                    pr_approvals=await github.get_pr_approvals(pr.number)
                )
                
                if merge_confidence > 0.85 and await all_checks_passed(pr.number):
                    # Auto-merge
                    await github.merge_pr(
                        pr.number,
                        merge_method="squash",
                        commit_message=generate_merge_commit_message(pr_description)
                    )
                    
                    # Set up post-merge monitoring
                    await setup_production_monitoring(
                        pr.number,
                        implementation.files_changed,
                        monitoring_duration_hours=24
                    )
                    
                    result_status = "auto_merged"
                else:
                    # Request human review
                    await github.request_review(
                        pr.number,
                        reviewers=get_relevant_reviewers(implementation.files_changed)
                    )
                    
                    await github.comment(
                        pr.number,
                        f"""🤔 This PR needs human review before merging.
                        
                        Confidence: {merge_confidence:.0%}
                        Reasons: {get_human_review_reasons(merge_confidence, self_review)}
                        
                        You can ask @claude for clarification on any changes."""
                    )
                    
                    result_status = "awaiting_human_review"
                
                # 7. Store outcome for learning
                await learning_db.store_pr_outcome(
                    pr.number,
                    implementation,
                    test_validation,
                    merge_confidence,
                    result_status
                )
                
                return PRResult(
                    pr_number=pr.number,
                    pr_url=pr.html_url,
                    status=result_status,
                    merge_confidence=merge_confidence,
                    self_review_issues=len(self_review.issues_found),
                    auto_fixes_applied=len(fixes) if 'fixes' in locals() else 0
                )
            
            async def calculate_merge_confidence(implementation, test_validation, self_review, pr_approvals):
                """Calculate confidence for auto-merge decision"""
                
                factors = {
                    "implementation_confidence": implementation.confidence,
                    "test_confidence": test_validation.confidence,
                    "test_coverage": min(test_validation.coverage / 80, 1.0),  # 80% is ideal
                    "self_review_clean": 1.0 if not self_review.issues_found else 0.5,
                    "production_risk": 1.0 - test_validation.production_risk,
                    "has_approvals": 1.0 if pr_approvals else 0.7,
                    "complexity_factor": 1.0 - (implementation.complexity * 0.5)
                }
                
                weights = {
                    "implementation_confidence": 0.25,
                    "test_confidence": 0.25,
                    "test_coverage": 0.15,
                    "self_review_clean": 0.1,
                    "production_risk": 0.15,
                    "has_approvals": 0.05,
                    "complexity_factor": 0.05
                }
                
                return sum(factors[k] * weights[k] for k in factors)
            ```
        labels:
          - "sprint-6"
          - "phase-2"
          - "oauth-migration"
          - "pr-manager"
        dependencies:
          - "Migrate test-runner to OAuth-based Modal"
        estimate: "10 hours"
        assignee: "devops-engineer"

  - phase: 3
    name: "Autonomous Pipeline Orchestration"
    status: pending
    priority: high
    component: orchestration
    description: "Build complete autonomous orchestration with learning feedback loops"
    duration_days: 7
    tasks:
      - title: "Build Autonomous Pipeline Orchestrator with Learning Loops"
        description: |
          Create master orchestrator that coordinates autonomous agents,
          manages confidence thresholds, and implements learning feedback.

          ## Acceptance Criteria
          - [ ] End-to-end autonomous pipeline orchestration
          - [ ] Confidence-based decision routing
          - [ ] Parallel agent execution where possible
          - [ ] Real-time learning and threshold adjustment
          - [ ] Human escalation with context preservation
          - [ ] Performance metrics and optimization

          ## Implementation Notes
            ```python
            @modal.function(
                secrets=[modal.Secret.from_name("claude-oauth-tokens")],
                image=modal.Image.debian_slim().pip_install(
                    "anthropic", "pydantic", "numpy", "asyncio"
                ),
                timeout=3600  # 1 hour for complete workflow
            )
            async def autonomous_pipeline_orchestrator(
                issue: dict,
                classification: IssueClassification,
                command: str = None
            ) -> PipelineResult:
                """Master orchestrator for autonomous issue resolution"""
                
                claude = ClaudeOAuthClient(oauth_token)
                learning_db = LearningDatabase()
                
                # Initialize pipeline state
                pipeline_state = PipelineState(
                    issue_id=issue["number"],
                    start_time=datetime.utcnow(),
                    confidence_threshold=0.7,
                    max_retries=3
                )
                
                try:
                    # 1. Investigation Phase (can run in parallel with historical analysis)
                    investigation_task = asyncio.create_task(
                        autonomous_investigate_issue(issue, classification)
                    )
                    
                    historical_analysis_task = asyncio.create_task(
                        learning_db.analyze_historical_patterns(issue)
                    )
                    
                    investigation, historical = await asyncio.gather(
                        investigation_task,
                        historical_analysis_task
                    )
                    
                    pipeline_state.record_phase_completion(
                        "investigation",
                        investigation.confidence,
                        investigation.to_dict()
                    )
                    
                    # Check if we should continue autonomously
                    if investigation.needs_human_review:
                        return await escalate_to_human(
                            pipeline_state,
                            "Investigation requires human review",
                            investigation
                        )
                    
                    # 2. Implementation Phase (with self-healing)
                    implementation = await autonomous_implement(
                        investigation,
                        classification
                    )
                    
                    pipeline_state.record_phase_completion(
                        "implementation",
                        implementation.confidence,
                        implementation.to_dict()
                    )
                    
                    # Handle implementation failures with retry
                    if not implementation.success:
                        if pipeline_state.can_retry("implementation"):
                            # Try alternative approach based on learning
                            alternative_approach = await generate_alternative_strategy(
                                investigation,
                                implementation.partial_implementation,
                                historical
                            )
                            
                            implementation = await autonomous_implement(
                                investigation,
                                classification,
                                approach_override=alternative_approach
                            )
                            
                            if not implementation.success:
                                return await escalate_to_human(
                                    pipeline_state,
                                    "Implementation failed after retry",
                                    implementation
                                )
                    
                    # 3. Validation Phase (can partially run in parallel)
                    validation_tasks = [
                        autonomous_test_validation(implementation, investigation),
                        security_validation(implementation),
                        performance_validation(implementation)
                    ]
                    
                    validations = await asyncio.gather(*validation_tasks)
                    test_validation = validations[0]
                    
                    pipeline_state.record_phase_completion(
                        "validation",
                        test_validation.confidence,
                        test_validation.to_dict()
                    )
                    
                    # Handle test failures with implementation fixes
                    if test_validation.needs_implementation_fix:
                        if pipeline_state.can_retry("validation"):
                            # Self-heal implementation based on test failures
                            healing_implementation = await heal_implementation_from_tests(
                                implementation,
                                test_validation.bug_found
                            )
                            
                            # Re-run validation
                            test_validation = await autonomous_test_validation(
                                healing_implementation,
                                investigation
                            )
                            
                            if not test_validation.success:
                                return await escalate_to_human(
                                    pipeline_state,
                                    "Tests still failing after healing attempt",
                                    test_validation
                                )
                    
                    # 4. PR Creation and Management Phase
                    pr_result = await autonomous_pr_manager(
                        implementation,
                        test_validation,
                        investigation
                    )
                    
                    pipeline_state.record_phase_completion(
                        "pr_creation",
                        pr_result.merge_confidence,
                        pr_result.to_dict()
                    )
                    
                    # 5. Learning Phase (async, non-blocking)
                    asyncio.create_task(
                        record_pipeline_outcome(
                            pipeline_state,
                            pr_result,
                            learning_db
                        )
                    )
                    
                    # 6. Adjust thresholds based on outcome
                    if pr_result.status == "auto_merged":
                        # Success - can be slightly more aggressive
                        await adjust_confidence_thresholds(
                            learning_db,
                            direction="increase",
                            factor=1.02
                        )
                    elif pr_result.status == "awaiting_human_review":
                        # Needed human - be more conservative
                        await adjust_confidence_thresholds(
                            learning_db,
                            direction="decrease",
                            factor=0.98
                        )
                    
                    # Calculate overall pipeline metrics
                    pipeline_metrics = calculate_pipeline_metrics(pipeline_state)
                    
                    return PipelineResult(
                        success=True,
                        issue_id=issue["number"],
                        pr_number=pr_result.pr_number if pr_result else None,
                        total_time=pipeline_state.elapsed_time,
                        confidence_score=pipeline_state.overall_confidence,
                        autonomous_completion=pr_result.status == "auto_merged",
                        phases_completed=pipeline_state.completed_phases,
                        self_healing_count=pipeline_state.total_healing_attempts,
                        metrics=pipeline_metrics
                    )
                    
                except Exception as e:
                    # Graceful failure with learning
                    await record_pipeline_failure(
                        pipeline_state,
                        e,
                        learning_db
                    )
                    
                    return await escalate_to_human(
                        pipeline_state,
                        f"Pipeline error: {str(e)}",
                        {"error": str(e), "traceback": traceback.format_exc()}
                    )
            
            async def escalate_to_human(pipeline_state, reason, context):
                """Gracefully escalate to human with full context"""
                
                # Create comprehensive context document
                context_doc = await generate_escalation_context(
                    pipeline_state,
                    reason,
                    context
                )
                
                # Create GitHub issue comment with context
                await github.comment(
                    pipeline_state.issue_id,
                    f"""🤖 I need human assistance to complete this issue.
                    
                    **Reason**: {reason}
                    **Progress**: {pipeline_state.progress_summary}
                    **Confidence**: {pipeline_state.overall_confidence:.0%}
                    
                    <details>
                    <summary>Full Context</summary>
                    
                    {context_doc}
                    </details>
                    
                    You can pick up where I left off or provide guidance via @claude mentions.
                    """
                )
                
                return PipelineResult(
                    success=False,
                    needs_human_intervention=True,
                    escalation_reason=reason,
                    partial_progress=pipeline_state.to_dict()
                )
            ```
        labels:
          - "sprint-6"
          - "phase-3"
          - "oauth-orchestration"
          - "natural-language"
        dependencies:
          - "Migrate pr-manager to OAuth-based Modal"
        estimate: "16 hours"
        assignee: "architecture-lead"

      - title: "Build Continuous Learning and Model Training System"
        description: |
          Create continuous learning system that improves the pipeline
          based on outcomes and human feedback.

          ## Acceptance Criteria
          - [ ] Automated outcome collection and analysis
          - [ ] Model retraining pipeline for classification
          - [ ] Threshold adjustment based on success rates
          - [ ] Pattern recognition for common failures
          - [ ] Prompt optimization system
          - [ ] Performance dashboard and metrics

          ## Implementation Notes
            ```python
            @modal.function(
                schedule=modal.Period(hours=6),  # Run 4 times daily
                secrets=[modal.Secret.from_name("claude-oauth-tokens")],
                image=modal.Image.debian_slim().pip_install(
                    "scikit-learn", "pandas", "numpy", "anthropic", "plotly"
                ),
                gpu="T4"  # For model training
            )
            async def continuous_learning_pipeline():
                """Continuously improve the autonomous pipeline based on outcomes"""
                
                learning_db = LearningDatabase()
                claude = ClaudeOAuthClient(oauth_token)
                
                # 1. Collect recent outcomes (past 24 hours)
                recent_outcomes = await learning_db.get_recent_outcomes(
                    hours=24,
                    include_human_feedback=True
                )
                
                # 2. Analyze success patterns
                success_analysis = await analyze_success_patterns(recent_outcomes)
                
                # 3. Identify failure patterns
                failure_patterns = await identify_failure_patterns(
                    recent_outcomes,
                    min_occurrences=3  # Need 3+ similar failures to identify pattern
                )
                
                # 4. Retrain classification model if needed
                if should_retrain_classifier(recent_outcomes):
                    new_classifier = await retrain_issue_classifier(
                        recent_outcomes,
                        current_model=load_current_classifier()
                    )
                    
                    # Validate new model
                    validation_score = await validate_classifier(
                        new_classifier,
                        test_set=recent_outcomes[-100:]  # Last 100 issues
                    )
                    
                    if validation_score > current_model_score:
                        await deploy_new_classifier(new_classifier)
                        print(f"Deployed new classifier with score: {validation_score}")
                
                # 5. Adjust confidence thresholds
                threshold_adjustments = calculate_threshold_adjustments(
                    success_analysis,
                    failure_patterns
                )
                
                await apply_threshold_adjustments(threshold_adjustments)
                
                # 6. Optimize prompts based on outcomes
                prompt_improvements = await optimize_prompts(
                    recent_outcomes,
                    claude
                )
                
                for improvement in prompt_improvements:
                    await test_and_deploy_prompt(improvement)
                
                # 7. Generate learning report
                report = await generate_learning_report(
                    recent_outcomes,
                    success_analysis,
                    failure_patterns,
                    threshold_adjustments,
                    prompt_improvements
                )
                
                # 8. Update metrics dashboard
                await update_metrics_dashboard(report)
                
                return report
            
            async def analyze_success_patterns(outcomes):
                """Identify what makes issues succeed autonomously"""
                
                successful_outcomes = [o for o in outcomes if o["autonomous_completion"]]
                
                # Extract features from successful cases
                success_features = extract_features_from_outcomes(successful_outcomes)
                
                # Cluster to find patterns
                clusters = cluster_outcomes(success_features)
                
                patterns = []
                for cluster in clusters:
                    pattern = {
                        "cluster_size": len(cluster),
                        "common_features": identify_common_features(cluster),
                        "avg_confidence": np.mean([o["confidence"] for o in cluster]),
                        "avg_time": np.mean([o["resolution_time"] for o in cluster]),
                        "example_issues": cluster[:3]  # Top 3 examples
                    }
                    patterns.append(pattern)
                
                return patterns
            
            async def optimize_prompts(outcomes, claude):
                """Use AI to improve prompts based on outcomes"""
                
                # Group outcomes by prompt type
                prompt_groups = group_by_prompt_type(outcomes)
                
                improvements = []
                for prompt_type, group_outcomes in prompt_groups.items():
                    if len(group_outcomes) < 10:
                        continue  # Need enough data
                    
                    # Analyze what worked and what didn't
                    analysis = await claude.chat([
                        {"role": "system", "content": "You are analyzing prompt effectiveness..."},
                        {"role": "user", "content": f"""
                        Analyze these outcomes for {prompt_type} prompts:
                        
                        Successful cases: {[o for o in group_outcomes if o["success"]]}
                        Failed cases: {[o for o in group_outcomes if not o["success"]]}
                        
                        Current prompt template: {get_current_prompt(prompt_type)}
                        
                        How can we improve this prompt to increase success rate?
                        """}
                    ])
                    
                    # Generate improved prompt variations
                    variations = await generate_prompt_variations(
                        analysis.content,
                        prompt_type,
                        current_prompt=get_current_prompt(prompt_type)
                    )
                    
                    improvements.append({
                        "prompt_type": prompt_type,
                        "current_success_rate": calculate_success_rate(group_outcomes),
                        "variations": variations,
                        "analysis": analysis.content
                    })
                
                return improvements
            
            async def generate_learning_report(outcomes, success_patterns, failure_patterns, adjustments, improvements):
                """Generate comprehensive learning report"""
                
                report = {
                    "period": "last_24_hours",
                    "total_issues": len(outcomes),
                    "autonomous_completion_rate": len([o for o in outcomes if o["autonomous_completion"]]) / len(outcomes),
                    "avg_resolution_time": np.mean([o["resolution_time"] for o in outcomes]),
                    "avg_confidence": np.mean([o["confidence"] for o in outcomes]),
                    "success_patterns": success_patterns,
                    "failure_patterns": failure_patterns,
                    "threshold_adjustments": adjustments,
                    "prompt_improvements": improvements,
                    "recommendations": await generate_recommendations(
                        success_patterns,
                        failure_patterns
                    )
                }
                
                # Create visualizations
                report["visualizations"] = {
                    "success_rate_trend": create_success_rate_chart(outcomes),
                    "confidence_distribution": create_confidence_histogram(outcomes),
                    "resolution_time_by_complexity": create_time_complexity_scatter(outcomes),
                    "failure_heatmap": create_failure_pattern_heatmap(failure_patterns)
                }
                
                return report
            ```
        labels:
          - "sprint-6"
          - "phase-3"
          - "code-generation"
          - "oauth-implementation"
        dependencies:
          - "Implement OAuth Orchestrator with Natural Language Processing"
        estimate: "14 hours"
        assignee: "backend-engineer"

  - phase: 4
    name: "OAuth Integration & Testing"
    status: pending
    priority: high
    component: integration
    description: "Complete OAuth system integration and production readiness"
    duration_days: 7
    tasks:
      - title: "End-to-End OAuth Workflow Testing"
        description: |
          Test complete OAuth-based workflow from @claude mention to
          PR creation without any API keys.

          ## Acceptance Criteria
          - [ ] Complete workflow test using only OAuth tokens
          - [ ] @claude mention triggers full workflow
          - [ ] Natural language commands processed correctly
          - [ ] OAuth token refresh during long workflows
          - [ ] Performance comparison with API key approach

          ## Implementation Notes
            - Test workflow: Issue → @claude investigate → implement → test → PR
            - Validate OAuth token handling across all agents
            - Test token expiration and refresh scenarios
            - Measure Claude API usage within subscription limits
            - Document OAuth setup process for team
        labels:
          - "sprint-6"
          - "phase-4"
          - "oauth-testing"
          - "integration"
        dependencies:
          - "Add Implementation Agent with Claude Code Generation"
        estimate: "12 hours"
        assignee: "qa-engineer"

      - title: "OAuth Security Audit and Best Practices"
        description: |
          Security review of OAuth implementation and credential
          management across Modal infrastructure.

          ## Acceptance Criteria
          - [ ] OAuth token storage security validated
          - [ ] Token refresh mechanism security audit
          - [ ] No hardcoded credentials anywhere
          - [ ] Audit logging for all OAuth operations
          - [ ] Documentation of OAuth security best practices
          - [ ] Penetration testing of OAuth endpoints
          - [ ] Security incident response plan

          ## Implementation Notes
            - Validate Modal Secrets encryption for OAuth tokens
            - Ensure tokens never logged or exposed
            - Implement token rotation schedule
            - Create OAuth troubleshooting guide
            - Document Claude subscription limit monitoring
            - Set up security monitoring alerts

      - title: "Implement Zero-Downtime Migration and Rollback Strategy"
        description: |
          Create comprehensive migration plan with feature flags and
          instant rollback capabilities.

          ## Acceptance Criteria
          - [ ] Feature flag system for gradual migration
          - [ ] Parallel running of old and new systems
          - [ ] Traffic routing based on feature flags
          - [ ] One-click rollback mechanism
          - [ ] Data consistency validation
          - [ ] Performance comparison dashboard
          - [ ] Migration runbook with checkpoints

          ## Implementation Notes
            ```python
            from enum import Enum
            import json
            from typing import Dict, Any
            
            class MigrationStage(Enum):
                LEGACY_ONLY = "legacy_only"
                CANARY_5_PERCENT = "canary_5"
                CANARY_25_PERCENT = "canary_25"
                CANARY_50_PERCENT = "canary_50"
                MODAL_PRIMARY = "modal_primary"
                MODAL_ONLY = "modal_only"
            
            class MigrationController:
                def __init__(self):
                    self.redis_client = get_redis_client()
                    self.metrics_client = get_metrics_client()
                    
                async def get_current_stage(self) -> MigrationStage:
                    stage = await self.redis_client.get("migration:stage")
                    return MigrationStage(stage) if stage else MigrationStage.LEGACY_ONLY
                
                async def route_request(self, request_context: Dict[str, Any]):
                    stage = await self.get_current_stage()
                    
                    # Check for forced routing (testing)
                    if force_route := request_context.get("force_route"):
                        return force_route
                    
                    # Determine routing based on stage
                    if stage == MigrationStage.LEGACY_ONLY:
                        return "legacy"
                    elif stage == MigrationStage.MODAL_ONLY:
                        return "modal"
                    else:
                        # Canary routing based on hash
                        user_hash = hash(request_context["user_id"]) % 100
                        canary_threshold = {
                            MigrationStage.CANARY_5_PERCENT: 5,
                            MigrationStage.CANARY_25_PERCENT: 25,
                            MigrationStage.CANARY_50_PERCENT: 50,
                            MigrationStage.MODAL_PRIMARY: 95
                        }.get(stage, 0)
                        
                        route = "modal" if user_hash < canary_threshold else "legacy"
                        
                        # Track routing decision
                        await self.metrics_client.increment(
                            f"migration.routing.{route}",
                            tags={"stage": stage.value}
                        )
                        
                        return route
                
                async def advance_stage(self) -> bool:
                    current = await self.get_current_stage()
                    
                    # Check health metrics before advancing
                    if not await self._check_health_metrics():
                        return False
                    
                    # Advance to next stage
                    stages = list(MigrationStage)
                    current_index = stages.index(current)
                    
                    if current_index < len(stages) - 1:
                        next_stage = stages[current_index + 1]
                        await self.redis_client.set("migration:stage", next_stage.value)
                        
                        # Log stage transition
                        await self._log_stage_transition(current, next_stage)
                        return True
                    
                    return False
                
                async def rollback(self, target_stage: MigrationStage = None):
                    current = await self.get_current_stage()
                    target = target_stage or MigrationStage.LEGACY_ONLY
                    
                    # Immediate rollback
                    await self.redis_client.set("migration:stage", target.value)
                    
                    # Alert on rollback
                    await self._send_rollback_alert(current, target)
                    
                    # Preserve state for analysis
                    await self._capture_rollback_state(current, target)
                
                async def _check_health_metrics(self) -> bool:
                    # Check error rates, latency, success rates
                    metrics = await self.metrics_client.query({
                        "error_rate": "rate(errors[5m])",
                        "p99_latency": "histogram_quantile(0.99, latency[5m])",
                        "success_rate": "rate(success[5m])"
                    })
                    
                    return (
                        metrics["error_rate"] < 0.01 and  # <1% errors
                        metrics["p99_latency"] < 2000 and  # <2s p99
                        metrics["success_rate"] > 0.99      # >99% success
                    )
            
            # Usage in webhook handler
            @modal.web_endpoint(method="POST")
            async def github_webhook_with_migration(request: Request):
                controller = MigrationController()
                
                # Determine routing
                route = await controller.route_request({
                    "user_id": request.headers.get("X-GitHub-Delivery"),
                    "endpoint": "webhook"
                })
                
                if route == "modal":
                    return await github_webhook_modal(request)
                else:
                    return await github_webhook_legacy(request)
            ```
        labels:
          - "sprint-6"
          - "phase-4"
          - "oauth-security"
          - "production-ready"
        dependencies:
          - "End-to-End OAuth Workflow Testing"
        estimate: "10 hours"
        assignee: "security-engineer"

      - title: "Implement Comprehensive Monitoring and Observability"
        description: |
          Set up monitoring, alerting, and observability for the Modal
          OAuth infrastructure with real-time dashboards.

          ## Acceptance Criteria
          - [ ] OpenTelemetry integration for distributed tracing
          - [ ] Prometheus metrics for all key operations
          - [ ] Grafana dashboards for system health
          - [ ] PagerDuty integration for critical alerts
          - [ ] Cost tracking and budget alerts
          - [ ] Performance baseline establishment
          - [ ] SLO/SLA monitoring setup

          ## Implementation Notes
            ```python
            from opentelemetry import trace, metrics
            from opentelemetry.exporter.otlp.proto.grpc import (
                trace_exporter, metrics_exporter
            )
            import time
            from functools import wraps
            
            # Initialize telemetry
            tracer = trace.get_tracer("modal-oauth-workflow")
            meter = metrics.get_meter("modal-oauth-workflow")
            
            # Create metrics
            workflow_counter = meter.create_counter(
                "workflow_executions",
                description="Number of workflow executions"
            )
            
            oauth_refresh_histogram = meter.create_histogram(
                "oauth_token_refresh_duration",
                description="OAuth token refresh duration",
                unit="ms"
            )
            
            api_call_histogram = meter.create_histogram(
                "claude_api_call_duration",
                description="Claude API call duration",
                unit="ms"
            )
            
            def monitor_function(operation_name: str):
                def decorator(func):
                    @wraps(func)
                    async def wrapper(*args, **kwargs):
                        # Start trace span
                        with tracer.start_as_current_span(
                            operation_name,
                            attributes={
                                "function": func.__name__,
                                "modal.function": True
                            }
                        ) as span:
                            start_time = time.time()
                            
                            try:
                                # Execute function
                                result = await func(*args, **kwargs)
                                
                                # Record success metrics
                                workflow_counter.add(
                                    1,
                                    {"status": "success", "operation": operation_name}
                                )
                                
                                return result
                                
                            except Exception as e:
                                # Record error
                                span.record_exception(e)
                                span.set_status(trace.Status(trace.StatusCode.ERROR))
                                
                                workflow_counter.add(
                                    1,
                                    {"status": "error", "operation": operation_name}
                                )
                                
                                raise
                                
                            finally:
                                # Record duration
                                duration_ms = (time.time() - start_time) * 1000
                                api_call_histogram.record(
                                    duration_ms,
                                    {"operation": operation_name}
                                )
                    
                    return wrapper
                return decorator
            
            # Cost tracking
            class CostTracker:
                def __init__(self):
                    self.cost_meter = meter.create_counter(
                        "estimated_cost_usd",
                        description="Estimated cost in USD"
                    )
                    
                async def track_modal_invocation(self, cpu_hours: float, memory_gb_hours: float):
                    # Modal pricing (example rates)
                    cpu_cost = cpu_hours * 0.00001  # $0.00001 per CPU millisecond
                    memory_cost = memory_gb_hours * 0.0000015  # $0.0000015 per GB millisecond
                    
                    total_cost = cpu_cost + memory_cost
                    
                    self.cost_meter.add(
                        total_cost,
                        {
                            "service": "modal",
                            "resource": "compute"
                        }
                    )
                
                async def track_claude_api_usage(self, model: str, input_tokens: int, output_tokens: int):
                    # Claude pricing per 1K tokens (example rates)
                    rates = {
                        "claude-3-opus": {"input": 0.015, "output": 0.075},
                        "claude-3-sonnet": {"input": 0.003, "output": 0.015},
                        "claude-3-haiku": {"input": 0.00025, "output": 0.00125}
                    }
                    
                    rate = rates.get(model, rates["claude-3-sonnet"])
                    cost = (input_tokens * rate["input"] + output_tokens * rate["output"]) / 1000
                    
                    self.cost_meter.add(
                        cost,
                        {
                            "service": "claude",
                            "model": model,
                            "operation": "chat"
                        }
                    )
            
            # Grafana dashboard config (as code)
            dashboard_config = {
                "title": "Modal OAuth Workflow Dashboard",
                "panels": [
                    {
                        "title": "Workflow Success Rate",
                        "query": "rate(workflow_executions{status='success'}[5m]) / rate(workflow_executions[5m])"
                    },
                    {
                        "title": "OAuth Token Refresh Latency",
                        "query": "histogram_quantile(0.99, oauth_token_refresh_duration)"
                    },
                    {
                        "title": "Cost per Hour",
                        "query": "sum(rate(estimated_cost_usd[1h])) * 3600"
                    },
                    {
                        "title": "Active Workflows",
                        "query": "sum(up{job='modal-workflow'})"
                    }
                ]
            }
            ```
        labels:
          - "sprint-6"
          - "phase-4"
          - "monitoring"
          - "observability"
        dependencies:
          - "Implement Zero-Downtime Migration and Rollback Strategy"
        estimate: "14 hours"
        assignee: "devops-engineer"

team:
  - role: lead
    agent: architecture_agent
    responsibilities:
      - "OAuth architecture design and oversight"
      - "Modal infrastructure with OAuth integration"
      - "Natural language processing design"

  - role: implementation
    agent: backend_agent
    responsibilities:
      - "Claude OAuth client implementation"
      - "Agent OAuth migration"
      - "@claude mention handling"

  - role: ai_engineering
    agent: ai_agent
    responsibilities:
      - "Claude SDK OAuth integration"
      - "Natural language command processing"
      - "Prompt optimization for OAuth context"

  - role: validation
    agent: qa_agent
    responsibilities:
      - "OAuth workflow testing"
      - "Token refresh validation"
      - "Subscription limit monitoring"

  - role: security
    agent: security_agent
    responsibilities:
      - "OAuth security audit"
      - "Credential management validation"
      - "Token rotation implementation"

success_metrics:
  - metric: autonomous_completion_rate
    target: 80
    unit: percent
    description: "Issues resolved without human intervention"

  - metric: time_to_resolution_reduction
    target: 50
    unit: percent
    description: "Reduction in average issue resolution time"

  - metric: self_healing_success_rate
    target: 75
    unit: percent
    description: "Percentage of failures that self-heal successfully"

  - metric: confidence_prediction_accuracy
    target: 90
    unit: percent
    description: "Accuracy of confidence scoring predictions"

  - metric: auto_merge_success_rate
    target: 95
    unit: percent
    description: "Success rate of auto-merged PRs in production"

  - metric: learning_improvement_rate
    target: 5
    unit: percent_per_month
    description: "Monthly improvement in autonomous completion"

  - metric: test_generation_coverage
    target: 85
    unit: percent
    description: "Code coverage from auto-generated tests"

  - metric: human_escalation_resolution
    target: 90
    unit: percent
    description: "Issues resolved after single human escalation"

  - metric: production_incident_rate
    target: 0.1
    unit: percent
    description: "Production incidents from autonomous changes"

  - metric: cost_per_issue_reduction
    target: 70
    unit: percent
    description: "Cost reduction per issue resolved"

  - metric: parallel_issue_capacity
    target: 50
    unit: issues
    description: "Concurrent autonomous issue resolution"

  - metric: pattern_recognition_accuracy
    target: 85
    unit: percent
    description: "Accuracy in identifying similar past issues"

graph_metadata:
  node_type: sprint_document
  relationships:
    - type: modernizes
      target: workflow_issue_system
    - type: implements
      target: oauth_authentication_pattern
    - type: enables
      target: natural_language_workflows
    - type: eliminates
      target: api_key_management

config:
  oauth_config:
    client_id: "9d1c250a-e61b-44d9-88ed-5944d1962f5e"
    scopes: ["read:user", "read:org", "api:access"]
    refresh_interval_hours: 4

  webhook_config:
    events: ["issues", "issue_comment", "pull_request", "pull_request_review_comment"]
    mention_pattern: "@claude\\s+(.+)"

  auto_create_issues: true
  issue_template: "sprint-task.md"
  default_labels:
    - "sprint-6"
    - "modal-oauth"
    - "architecture"
  milestone: "Sprint 6 - Modal OAuth Migration"

# ------------------------------------------------------------------
# References
# Modal Secret reference :contentReference[oaicite:0]{index=0}
# Modal webhooks guide :contentReference[oaicite:1]{index=1}
# Modal web endpoint example :contentReference[oaicite:2]{index=2}
# Anthropic IAM & OAuth concepts :contentReference[oaicite:3]{index=3}
# Anthropic SDK GitHub example :contentReference[oaicite:4]{index=4}
# Claude SDK documentation :contentReference[oaicite:5]{index=5}
# Anthropic Client SDKs overview :contentReference[oaicite:6]{index=6}
# Modal Secrets guide :contentReference[oaicite:7]{index=7}
# Modal Hello World proxy auth sample :contentReference[oaicite:8]{index=8}
# Modal OAuth tokens article :contentReference[oaicite:9]{index=9}
